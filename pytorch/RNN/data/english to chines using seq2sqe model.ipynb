{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "235e888a",
   "metadata": {},
   "source": [
    "# Machine Translation using Sequence to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfb39c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cee98f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "805f577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# readin dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d7318a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>chin</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>嗨。</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>你好。</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run.</td>\n",
       "      <td>你用跑的。</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wait!</td>\n",
       "      <td>等等！</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wait!</td>\n",
       "      <td>等一下！</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     eng   chin                                               info\n",
       "0    Hi.     嗨。  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
       "1    Hi.    你好。  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
       "2   Run.  你用跑的。  CC-BY 2.0 (France) Attribution: tatoeba.org #4...\n",
       "3  Wait!    等等！  CC-BY 2.0 (France) Attribution: tatoeba.org #1...\n",
       "4  Wait!   等一下！  CC-BY 2.0 (France) Attribution: tatoeba.org #1..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_table(\"./data/cmn.txt\", names=['eng', 'chin', 'info'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ad74e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24026, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting shape of dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9b52942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do not need info so only taking eng and chin features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f15bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"eng\", \"chin\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "095eefe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>chin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>嗨。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>你好。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run.</td>\n",
       "      <td>你用跑的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wait!</td>\n",
       "      <td>等等！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wait!</td>\n",
       "      <td>等一下！</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     eng   chin\n",
       "0    Hi.     嗨。\n",
       "1    Hi.    你好。\n",
       "2   Run.  你用跑的。\n",
       "3  Wait!    等等！\n",
       "4  Wait!   等一下！"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0342636",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "We are folowing some of the give preprocessing steps:\n",
    "* Removing punctuations like . , ! $( ) * % @\n",
    "* Removing URLs\n",
    "* Removing Stop words\n",
    "* Lower casing\n",
    "* Tokenization\n",
    "* Stemming\n",
    "* Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd96bc72",
   "metadata": {},
   "source": [
    "#### Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05da0e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'|', '$', ';', '@', '\\\\', '*', '#', '<', '!', '?', '+', '.', ')', '[', '&', \"'\", '_', '{', '~', '-', '>', '\"', ']', '}', ',', ':', '^', '(', '%', '=', '`', '/'}\n"
     ]
    }
   ],
   "source": [
    "# getting punctuation form string\n",
    "set_of_punctuation = set(string.punctuation)\n",
    "print(set_of_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b93a4483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to remove punctuation form whole dataset\n",
    "def remove_punctuation(text):\n",
    "    punctuation_free_text = ''.join([char for char in text if char not in set_of_punctuation])\n",
    "    return punctuation_free_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2c28a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Hi\n",
       "1      Hi\n",
       "2     Run\n",
       "3    Wait\n",
       "4    Wait\n",
       "Name: eng, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "temp = df[\"eng\"].apply(lambda x: remove_punctuation(x))\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7984d2",
   "metadata": {},
   "source": [
    "#### Removing Uneven Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03971cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to remove uneven spaces form text\n",
    "def remove_uneven_spaces(text):\n",
    "    uneven_space_free = re.sub(r\"\\s+\", ' ', text.strip())\n",
    "    return uneven_space_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be865af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    This is an example text with uneven spaces.\n",
       "1    This is an example text with uneven spaces.\n",
       "2    This is an example text with uneven spaces.\n",
       "3    This is an example text with uneven spaces.\n",
       "4    This is an example text with uneven spaces.\n",
       "Name: eng, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "text = \"  This  is   an example    text   with uneven   spaces.  \"\n",
    "result = df[\"eng\"].apply(lambda x: remove_uneven_spaces(text))\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e018af",
   "metadata": {},
   "source": [
    "#### Lower casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bba9ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a functio to convert text into lower case\n",
    "def convert_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43258fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "text = \"My Name \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80aaaff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing all process for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12fb96b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"eng\"] = df[\"eng\"].apply(lambda x: convert_lowercase(x))\n",
    "df[\"chin\"] = df[\"chin\"].apply(lambda x: convert_lowercase(x))\n",
    "\n",
    "df[\"eng\"] = df[\"eng\"].apply(lambda x: remove_punctuation(x))\n",
    "df[\"chin\"] = df[\"chin\"].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "df[\"eng\"] = df[\"eng\"].apply(lambda x: remove_uneven_spaces(x))\n",
    "df[\"chin\"] = df[\"chin\"].apply(lambda x: remove_uneven_spaces(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa16c1",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "Tokenizing the English and the Chinese words in to set all_english_vocabs and all_chinese_vocabs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3aa797b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_english_vocabs = set()\n",
    "for english in df[\"eng\"]:\n",
    "    words = english.split()\n",
    "    for word in words:\n",
    "        if word not in all_english_vocabs:\n",
    "            all_english_vocabs.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d322c0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chinese_vocabs = set()\n",
    "for chinese in df[\"chin\"]:\n",
    "    words = chinese.split()\n",
    "    for word in words:\n",
    "        if word not in all_chinese_vocabs:\n",
    "            all_chinese_vocabs.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2df5bcd",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "\n",
    "Here, we are going to build encoder and decoder models seperatly and ensembel them to form seq2seq model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad13900",
   "metadata": {},
   "source": [
    "#### Building Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06d19d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout_p):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_p)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x.shape : (x.length, batch_size)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding : (x.length, batch_size, embed_dim)\n",
    "        \n",
    "        output, (hidden, cell)= self.lstm(embedding)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b475a5",
   "metadata": {},
   "source": [
    "#### Building Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61097223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embeddig_size, output_size, num_layers, dropout_p):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_size\n",
    "        self.input_dim = input_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x,hidden, cell):\n",
    "        # input = [batch size]\n",
    "        x = x.unsqueeze(0)\n",
    "        # x: [1, batch_size]\n",
    "        \n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.lsmt(embedding)\n",
    "        # output : [1, batch_size, hidden_size]\n",
    "        \n",
    "        prediction = self.fc(output)\n",
    "        # predction: [1, N, batch_size, length_of_vocab]\n",
    "        \n",
    "        prediction = prediction.squeeze(0)\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d2bff",
   "metadata": {},
   "source": [
    "#### Building Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f054f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqNet(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2SeqNet, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        # getting batch_size, target_len and vocab_length\n",
    "        batch_size = source[1]\n",
    "        target_len = source[0]\n",
    "        target_vocab_len = len(all_english_vocabs)\n",
    "        \n",
    "        # variable to hold output\n",
    "        outputs = torch.zeros(target_len, batch_size)\n",
    "        \n",
    "        # Encoder\n",
    "        hidden, cell = self.encoder(source)\n",
    "        \n",
    "        # Decoder\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        x = target[0]\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            best_prediction = output.argmax(1)\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            x = target[t] if teacher_force else top1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32a6a879",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m INPUT_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43menglish\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m)\n\u001b[0;32m      2\u001b[0m OUTPUT_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chines\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[0;32m      3\u001b[0m ENC_EMB_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(english.vocab)\n",
    "OUTPUT_DIM = len(chines.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e789fdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'INPUT_DIM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m      4\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m enc \u001b[38;5;241m=\u001b[39m Encoder(\u001b[43mINPUT_DIM\u001b[49m, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n\u001b[0;32m      7\u001b[0m dec \u001b[38;5;241m=\u001b[39m Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m Seq2Seq(enc, dec, device)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'INPUT_DIM' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "learning_rate = 0.001\n",
    "step = 0\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341cdc7",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4862f0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
